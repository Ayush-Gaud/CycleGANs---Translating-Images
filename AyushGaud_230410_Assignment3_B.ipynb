{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VDe2CfGe1C"
      },
      "source": [
        "# Part B (Neural Network from Scratch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcVTpevGpfs"
      },
      "source": [
        "You need to implement a neural network from scratch .This is a multiclass classification problem. No. of hidden layers depends on you but should be atleast 2.Remember to use activation function. You can add any other function of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1049,
      "metadata": {
        "id": "G79JnP4t4Eqw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1050,
      "metadata": {
        "id": "KmTgnciWFC0O"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y = to_categorical(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1051,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVPMgOfMFC3o",
        "outputId": "9c695d71-0981-4491-b8e7-620b2b0cdc7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(150, 4) (150, 3)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape,y.shape)    # three classes to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1052,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usrSV41k8AUJ",
        "outputId": "959723de-37d5-442f-b480-343b32341712"
      },
      "outputs": [],
      "source": [
        "input_size = X.shape[1]\n",
        "hidden_size1 = 16\n",
        "hidden_size2 = 32\n",
        "output_size = y.shape[1]\n",
        "\n",
        "def parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
        "    # define the parameters of your nn initially using random lib.\n",
        "  \n",
        "    # between input layer and 1st hidden layer.\n",
        "    w1 = np.random.randn(input_size, hidden_size1)    # 16 neurons in 1st hidden layer.\n",
        "    b1 = np.zeros(hidden_size1)\n",
        "\n",
        "    # between 1st hidden layer and 2nd hidden layer.\n",
        "    w2 = np.random.randn(hidden_size1, hidden_size2)    # 32 neurons in 2nd hidden layer.\n",
        "    b2 = np.zeros(hidden_size2)\n",
        "\n",
        "    # between 2nd hidden layer and output layer.\n",
        "    w3 = np.random.randn(hidden_size2, output_size)    # 3 neurons in output layer.\n",
        "    b3 = np.zeros(output_size)\n",
        "  \n",
        "    return w1,b1,w2,b2,w3,b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1053,
      "metadata": {
        "id": "mYYaNx3N8qJc"
      },
      "outputs": [],
      "source": [
        "#activation functions\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0.0, z)    #returns maximum of 0.0 and z.(whereas max returns maximum element of an array.)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))        # we subtract max(z) to maintain 'numerical stability' as said on\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)         # stackoverflow as it eventually cancels out and does not change the answer.\n",
        "                                                                # keepdims makes sure the result is broadcasted properly.(i.e correct dimensions)\n",
        "def sigmoid_derivative(z):\n",
        "    return z * (1 - z)   \n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)       # 0 or 1                                                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1054,
      "metadata": {
        "id": "08ynlWSI8M_L"
      },
      "outputs": [],
      "source": [
        "def forward(X, w1,b1, w2,b2 ,w3,b3):\n",
        "    # function for forward propagation\n",
        "\n",
        "    # 1st hidden layer\n",
        "    y1 = np.dot(X, w1) + b1   # input\n",
        "    z1 = sigmoid(y1)          # output\n",
        "\n",
        "    # 2nd hidden layer\n",
        "    y2 = np.dot(z1, w2) + b2  # input\n",
        "    z2 = relu(y2)             # output\n",
        "\n",
        "    # output layer\n",
        "    y3 = np.dot(z2, w3) + b3  # input\n",
        "    z3 = softmax(y3)          # output\n",
        "    \n",
        "    initial_values = (y1, z1, y2, z2, y3, z3)\n",
        "\n",
        "    return z3, initial_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1055,
      "metadata": {
        "id": "8Fm79jv18Otc"
      },
      "outputs": [],
      "source": [
        "def cost_funct(y_true, y_pred):\n",
        "    # function for cost func if necessary\n",
        "\n",
        "    # using categorical crossentropy as our output layer has softmax (ont hot encoded).\n",
        "    m = y_true.shape[0]     # number of examples\n",
        "\n",
        "    cost = -np.sum(y_true * np.log(y_pred)) / m\n",
        "    return cost\n",
        "\n",
        "# Source - https://neuralthreads.medium.com/categorical-cross-entropy-loss-the-most-important-loss-function-d3792151d05b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1056,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward(X, y, values, w1, w2, w3):\n",
        "    # function for backward propagation\n",
        "    m = y.shape[0]\n",
        "    y1, z1, y2, z2, y3, z3 = values\n",
        "\n",
        "    dy3 = z3-y\n",
        "    dw3 = np.dot(z2.T, dy3)/m\n",
        "    db3 = np.sum(dy3, axis=0)/m\n",
        "\n",
        "    dz2 = np.dot(dy3, w3.T)\n",
        "    dy2 = dz2 * relu_derivative(y2)\n",
        "    dw2 = np.dot(z1.T, dy2)/m\n",
        "    db2 = np.sum(dy2, axis=0)/m\n",
        "\n",
        "    dz1 = np.dot(dy2, w2.T)\n",
        "    dy1 = dz1 * sigmoid_derivative(z1)\n",
        "    dw1 = np.dot(X.T, dy1)/m\n",
        "    db1 = np.sum(dy1, axis=0)/m\n",
        "\n",
        "    gradients = (dw1, db1, dw2, db2, dw3, db3)\n",
        "\n",
        "    return gradients\n",
        "\n",
        "# Source - https://towardsdatascience.com/backpropagation-from-scratch-how-neural-networks-really-work-36ee4af202bf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1057,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_parameters(w1, b1, w2, b2, w3, b3, gradients, learning_rate):\n",
        "#FUNCTION TO UPDATE PARAMETERS USING GD\n",
        "    dW1, db1, dW2, db2, dW3, db3 = gradients\n",
        "    w1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    w2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    w3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "    return w1, b1, w2, b2, w3, b3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1058,
      "metadata": {
        "id": "HctPDjITIeUq"
      },
      "outputs": [],
      "source": [
        "# use Gradient descent as of now as an optimizer\n",
        "def gradient_descent(X, y, w1, b1, w2, b2, w3, b3, learning_rate, epochs):\n",
        "    for i in range(epochs):\n",
        "        z3, initial_values = forward(X, w1, b1, w2, b2, w3, b3)      # forward propagation.\n",
        "        \n",
        "        cost = cost_funct(y, z3)       # computing cost.\n",
        "        \n",
        "        gradients = backward(X, y, initial_values, w1, w2, w3)     # backward propagation.\n",
        "        \n",
        "        w1, b1, w2, b2, w3, b3 = update_parameters(w1, b1, w2, b2, w3, b3, gradients, learning_rate)  # updating weights and biases.\n",
        "\n",
        "    return cost, w1, b1, w2, b2, w3, b3\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1059,
      "metadata": {
        "id": "x7DOBhxNIxST"
      },
      "outputs": [],
      "source": [
        "def model(X, y, W1, b1, W2, b2, W3, b3, learning_rate, epochs, cost):\n",
        "    #function to train and build the whole model.\n",
        "    for epoch in range(epochs):\n",
        "        if epoch % 100 == 0:                      #printing cost after 100 epochs.\n",
        "            print(f\"Epoch {epoch}, Cost: {cost}\")\n",
        "          \n",
        "    z3_pred, final_values = forward(X, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "    y_pred = np.argmax(z3_pred,axis=1)          # can use indexes as our output is one hot encoded.\n",
        "    y_true = np.argmax(y, axis=1)\n",
        "\n",
        "    accuracy = np.mean(y_pred == y_true)\n",
        "    F1_score = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "    print(\"Accuracy :\",round(accuracy*100, 2),\"%\")\n",
        "    print(\"F1 Score (Weighted) :\",round(F1_score, 2))\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1060,
      "metadata": {},
      "outputs": [],
      "source": [
        "W1, b1, W2, b2, W3, b3 = parameters(input_size, hidden_size1, hidden_size2, output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1061,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1062,
      "metadata": {
        "id": "pH7yhg6LI-R4"
      },
      "outputs": [],
      "source": [
        "#write down the predictions and the f1 score finally\n",
        "learning_rate = 0.01\n",
        "epochs = 2000  \n",
        "\n",
        "cost, W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, y_train, W1, b1, W2, b2, W3, b3, learning_rate, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1063,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Cost: 0.07285230288803324\n",
            "Epoch 100, Cost: 0.07285230288803324\n",
            "Epoch 200, Cost: 0.07285230288803324\n",
            "Epoch 300, Cost: 0.07285230288803324\n",
            "Epoch 400, Cost: 0.07285230288803324\n",
            "Epoch 500, Cost: 0.07285230288803324\n",
            "Epoch 600, Cost: 0.07285230288803324\n",
            "Epoch 700, Cost: 0.07285230288803324\n",
            "Epoch 800, Cost: 0.07285230288803324\n",
            "Epoch 900, Cost: 0.07285230288803324\n",
            "Epoch 1000, Cost: 0.07285230288803324\n",
            "Epoch 1100, Cost: 0.07285230288803324\n",
            "Epoch 1200, Cost: 0.07285230288803324\n",
            "Epoch 1300, Cost: 0.07285230288803324\n",
            "Epoch 1400, Cost: 0.07285230288803324\n",
            "Epoch 1500, Cost: 0.07285230288803324\n",
            "Epoch 1600, Cost: 0.07285230288803324\n",
            "Epoch 1700, Cost: 0.07285230288803324\n",
            "Epoch 1800, Cost: 0.07285230288803324\n",
            "Epoch 1900, Cost: 0.07285230288803324\n",
            "Accuracy : 98.33 %\n",
            "F1 Score (Weighted) : 0.98\n"
          ]
        }
      ],
      "source": [
        "model(X_train, y_train, W1, b1, W2, b2, W3, b3, learning_rate, epochs, cost)   # training data accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1064,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Cost: 0.07285230288803324\n",
            "Epoch 100, Cost: 0.07285230288803324\n",
            "Epoch 200, Cost: 0.07285230288803324\n",
            "Epoch 300, Cost: 0.07285230288803324\n",
            "Epoch 400, Cost: 0.07285230288803324\n",
            "Epoch 500, Cost: 0.07285230288803324\n",
            "Epoch 600, Cost: 0.07285230288803324\n",
            "Epoch 700, Cost: 0.07285230288803324\n",
            "Epoch 800, Cost: 0.07285230288803324\n",
            "Epoch 900, Cost: 0.07285230288803324\n",
            "Epoch 1000, Cost: 0.07285230288803324\n",
            "Epoch 1100, Cost: 0.07285230288803324\n",
            "Epoch 1200, Cost: 0.07285230288803324\n",
            "Epoch 1300, Cost: 0.07285230288803324\n",
            "Epoch 1400, Cost: 0.07285230288803324\n",
            "Epoch 1500, Cost: 0.07285230288803324\n",
            "Epoch 1600, Cost: 0.07285230288803324\n",
            "Epoch 1700, Cost: 0.07285230288803324\n",
            "Epoch 1800, Cost: 0.07285230288803324\n",
            "Epoch 1900, Cost: 0.07285230288803324\n",
            "Accuracy : 100.0 %\n",
            "F1 Score (Weighted) : 1.0\n"
          ]
        }
      ],
      "source": [
        "model(X_test, y_test, W1, b1, W2, b2, W3, b3, learning_rate, epochs, cost)   # testing data accuracy\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
