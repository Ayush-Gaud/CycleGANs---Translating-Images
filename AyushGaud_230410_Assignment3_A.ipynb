{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg6hsKMq398n"
      },
      "source": [
        "# ASSIGNMENT 3\n",
        "PART A consists of theoretical questions. Objective answers will receive zero marks. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading and finding about them). Questions that need less explanation can be answered in 95-150 words. Please ensure the answers are well-written and thorough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFQN0GPrnSd3"
      },
      "source": [
        "# PART A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuKQGaqx2a5i"
      },
      "source": [
        "Q 1)What are optimizers in ML. Give some examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Optimizers are algorithms used to find the optimal set of parameters for a model during the training process. \n",
        "These algorithms adjust the weights and biases in the model iteratively until they converge on a minimum loss value.\n",
        "Examples being Adam, Stochastic Gradient Descent, Adagrad, Adadelta, Adamax, etc.\n",
        "Adam is the best among the adaptive optimizers in most of the cases which involves a combination of two gradient descent methodologies: \n",
        "Momentum and RMSProp. \n",
        "\n",
        "(Source - https://www.geeksforgeeks.org/adam-optimizer/ and https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8GPK5j_2n3P"
      },
      "source": [
        "Q 2)Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Gradient Descent is an optimization algorithm for finding out the local minima of a differentiable function, \n",
        "it helps us to find the best parameters(Weights and Biases) for the algorithm we are using for our model.\n",
        "Gradient Descent or Batch Gradient Descent, Stochastic Gradient Descent and Mini-Batch Gradient Descent differ \n",
        "in how much data we use to compute the gradients , depending upon the amount of data , we make a trade off \n",
        "between accuracy of parameters updates(weights and biases) and the time it takes to perform an update , so the \n",
        "three differ in number of updates made , on the basis of convergence and the amount of time they take.\n",
        "\n",
        "(1) Batch Gradient Descent : \n",
        "    It is a technique where all the data is taken into consideration while making a single update to the parameters \n",
        "    of our model.\n",
        "\n",
        "(2) Stochastic Gradient Descent : \n",
        "    It is a technique where one row or one input is taken at a time to make a single update to the parameters of \n",
        "    our model.\n",
        "\n",
        "(3) Mini-Batch Gradient Descent :\n",
        "    It is a technique where a batch is taken in consideration at a time to make a single update to the parameters \n",
        "    of our model, we can set the batch size according to us , we set the batch size and \n",
        "    (no. of batches = Total Rows in data/batch size), it is a mid way between Batch gradient descent and \n",
        "    Stochastic gradient descent.\n",
        "\n",
        "(Source - https://medium.com/@amannagrawall002/batch-vs-stochastic-vs-mini-batch-gradient-descent-techniques-7dfe6f963a6f#:~:text=The%20mini%20batch%20gradient%20descent,set%20the%20hyperparameter%20of%20batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AG5Rm7c2sAO"
      },
      "source": [
        "Q 3)Explain about Adam optimizer in detail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "The Adam optimizer, short for “Adaptive Moment Estimation,” is an iterative optimization algorithm used to minimize \n",
        "the loss function during the training of neural networks. Adam can be looked at as a combination of RMSprop and \n",
        "Stochastic Gradient Descent with momentum.\n",
        "\n",
        "Key Features of Adam Optimizer:\n",
        "(1) Adaptive Learning Rates: \n",
        "    Adam adjusts the learning rates for each parameter individually. It calculates a moving average* of the \n",
        "    first-order moments (the mean of gradients) and the second-order moments (the variance of gradients)\n",
        "    to scale the learning rates adaptively.\n",
        "    (*moving average is calculated by taking the average of a set of data points over a specific time period.)\n",
        "\n",
        "(2) Bias Correction: \n",
        "    To counteract the initialization bias in the first moments, Adam applies bias correction during the early \n",
        "    iterations of training. This ensures faster convergence and stabilizes the training process.\n",
        "\n",
        "(3) Low Memory Requirements: \n",
        "    Unlike some optimization algorithms that require storing a history of gradients for each parameter, \n",
        "    Adam only needs to maintain two moving averages per parameter. This makes it memory-efficient, especially \n",
        "    for large neural networks.\n",
        "\n",
        "Mathematical Aspect of Adam Optimizer: \n",
        "\n",
        "mt = βmt-1 + (1-β)[∂L/∂Wt]vt\n",
        "\n",
        "mt = Aggregate of gradients at time t [Current] (Initially, mt = 0) \n",
        "mt-1 = Aggregate of gradients at time t-1 [Previous] \n",
        "Wt = Weights at time t \n",
        "Wt+1 = Weights at time t+1 \n",
        "αt = Learning rate at time t  \n",
        "∂L = Derivative of Loss Function \n",
        "∂Wt = Derivative of weights at time t \n",
        "β = Moving average parameter (Constant, 0.9)\n",
        "Vt = Sum of the square of past gradients. [i.e sum(∂L/∂Wt-1)] (initially, Vt = 0)  \n",
        "\n",
        "(Source - https://www.shiksha.com/online-courses/articles/adam-optimizer-for-stochastic-gradient-descent/ and \n",
        "https://www.analyticsvidhya.com/blog/2023/09/what-is-adam-optimizer/#:~:text=The%20Adam%20optimizer%2C%20short%20for,Stochastic%20Gradient%20Descent%20with%20momentum.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0I65BBb2w84"
      },
      "source": [
        "Q 4)Explain the difference between Rmsprop and Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Rmsprop and Adam are both optimization algorithms used in training machine learning models, particularly neural\n",
        "networks. They are designed to adapt the learning rate during training to improve convergence.\n",
        "\n",
        "Adam stands for adaptive moment estimation and is a stochastic gradient descent optimization algorithm that uses \n",
        "an adaptive learning rate based on estimates of the first and second moments. It maintains 'exponential moving \n",
        "averages' of the weights and gradients, which it uses to scale the learning rate. In other words, Adam uses \n",
        "estimates of the mean and variance of the gradients to adaptively scale the learning rate during training, which \n",
        "can improve the speed and stability of the optimization process.\n",
        "\n",
        "[Exponential Moving Average is a type of Moving Average, which applies more weight to the most recent data points\n",
        "than those which happened in past. In other words, it is like giving more importance to the last experience or \n",
        "memories than to older ones, assuming those are represented by data points.]\n",
        "(Source - https://medium.com/analytics-vidhya/understanding-exponential-moving-averages-e3f020d9d13b)\n",
        "\n",
        "The RMSProp optimiztion algorithm is an extension of the famous stochastic gradient descent (SGD) algorithm. The \n",
        "key idea behind RMSProp is to scale the gradient of each weight in the model by dividing it by the root mean \n",
        "square (RMS) of the gradients of that weight. This helps prevent weights with high gradients from learning too \n",
        "quickly while allowing weights with low gradients to continue learning faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVYyIYOe2w_k"
      },
      "source": [
        "Q 5)What do you think is the best optimizer among all and Why? If you cannot come to conclusive answer, you must list them all and tell in which scenario the one is preferred.Also tell the disadvantages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Among all the available algorithms, there isn't any optimizer which can be termed as the best one. This is because \n",
        "optimizers are context dependent and can vary based on the specific problem, dataset, etc.\n",
        "\n",
        "Some of the optimizers which are often used are :\n",
        "\n",
        "(1) Stochastic Gradient Descent:\n",
        "    Scenarios where SGD may be a good choice:\n",
        "    (i) Large datasets: SGD can train machine learning models efficiently because it processes training examples\n",
        "        one at a time and does not require the entire dataset to be loaded into memory at once.\n",
        "    (ii) Simple implementation: SGD is a simple optimization algorithm, making it a good choice for prototyping \n",
        "         and experimentation.\n",
        "    (iii) Sparse data: SGD can be more efficient than batch gradient descent when training on sparse data because \n",
        "          it only processes a small number of non-zero data points at a time.\n",
        "          (Sparse data refers to datasets with many features with zero values)\n",
        "\n",
        "    The disadvantages being :\n",
        "    (i) High noise due to frequent updates with a single or few samples.\n",
        "    (ii) Less stable as it may oscillate around the optimal solution.\n",
        "\n",
        "(2) Adam :\n",
        "    Scenarios where Adam may be a good choice:\n",
        "    (i) When you want a fast and efficient optimization algorithm: Adam requires relatively little memory and \n",
        "        computation, making it a fast and efficient choice for training deep learning models.\n",
        "    (ii) When you have noisy or sparse gradients: Adam is well-suited for optimizing models with noisy or sparse \n",
        "         gradients, as it can use the additional information provided by the gradients to adapt the learning rate \n",
        "         on the fly.\n",
        "    (iii) When you want to try a \"plug-and-play\" optimization algorithm: Adam, a \"plug-and-play\" optimization \n",
        "          algorithm, requires relatively little tuning. It is a good choice if you want to train your model quickly.\n",
        "\n",
        "    The disadvantages being :\n",
        "    (i) While Adam converges rapidly in most scenarios, it can sometimes fail to converge to the optimal solution, \n",
        "        particularly in cases with noisy or sparse gradients.\n",
        "    (ii) Adam maintains two moving averages for each parameter (moments), which can result in higher memory usage \n",
        "         and computational overhead compared to simpler algorithms like SGD.\n",
        "\n",
        "(3) RMSProp :\n",
        "    Scenarios where RMSProp may be a good choice:\n",
        "    (i) If you are training a model with many parameters and are experiencing issues with the model diverging or \n",
        "        oscillating during training, RMSProp can help stabilize the training process by adjusting to the gradient.\n",
        "    (ii) If the learning rate is hard to tune, RMSProp can be effective because it scales the gradients, which can\n",
        "         help the optimization process converge more smoothly regardless of the learning rate.\n",
        "    (iii) If you train a model on a noisy or irregularly-shaped loss function, RMSProp can smooth out short-term \n",
        "          fluctuations and highlight long-term trends which help mitigate the effects of noise and allow the model \n",
        "          to converge more quickly.\n",
        "\n",
        "    The disadvantages being :\n",
        "    (i) RMSprop keeps track of squared gradients over time, which can lead to very small learning rates during \n",
        "        later stages of training, slowing down the optimization process.\n",
        "    (ii) RMSprop is sensitive to the initial learning rate setting, and choosing the wrong value can cause problems.\n",
        "    (iii) Unlike some other optimization methods, RMSprop lacks momentum, which can help the learning process by \n",
        "          accumulating past gradients.\n",
        "    \n",
        "(Source - https://www.kaggle.com/code/harpdeci/intuitive-explanation-of-sgd-adam-and-rmsprop and \n",
        "https://www.quora.com/What-are-some-of-the-disadvantages-of-using-an-Adam-optimiser-for-training-deep-neural-networks and \n",
        "https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/ and https://iq.opengenus.org/rmsprop/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKxnf5et2xDF"
      },
      "source": [
        "Q 6)What is overfitting and underfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Underfitting:\n",
        "A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to \n",
        "capture data complexities. It represents the inability of the model to learn the training data effectively result\n",
        "in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, \n",
        "especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly \n",
        "simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with \n",
        "enhanced feature representation, and less regularization.The underfitting model has High bias and low variance.\n",
        "\n",
        "Overfitting:\n",
        "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. \n",
        "When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our\n",
        "data set. And when testing with test data results in High variance. Then the model does not categorize the data \n",
        "correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear\n",
        "methods because these types of machine learning algorithms have more freedom in building the model based on the \n",
        "dataset and therefore they can really build unrealistic models. \n",
        "\n",
        "(Source - https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UCncA8M2xF5"
      },
      "source": [
        "Q 7)Explain the vanishing and exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Vanishing:\n",
        "During backpropagation, the calculation of (partial) derivatives/gradients in the weight update formula follows\n",
        "the Chain Rule, where gradients in earlier layers are the multiplication of gradients of later layers.As the \n",
        "gradients frequently become 'smaller' until they are close to zero, the new model weights (of the initial layers) \n",
        "will be virtually identical to the old weights without any updates. As a result, the gradient descent algorithm \n",
        "never converges to the optimal solution. This is known as the problem of vanishing gradients, and it’s one example\n",
        "of unstable behaviors of neural nets.\n",
        "\n",
        "Exploding: \n",
        "If the gradients get 'larger' or even NaN as our backpropagation progresses, we would end up with exploding gradients\n",
        "having big weight updates, leading to the divergence of the gradient descent algorithm.\n",
        "\n",
        "(Source - https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing#:~:text=Two%20opposite%20scenarios%20could%20happen,Vanishing%20and%20Exploding%20Gradients%2C%20respectively.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNW8Hmd52xIn"
      },
      "source": [
        "Q 8)Explain batch and layer normalization in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Batch Normalization : \n",
        "(1) Batch Normalization is a technique used to normalize the activations of a layer within a mini-batch during training.\n",
        "(2) It is commonly applied to intermediate layers of a neural network to help stabilize training and improve the \n",
        "    convergence of the model.\n",
        "(3) BN normalizes the activations by subtracting the mini-batch mean and dividing by the mini-batch standard \n",
        "    deviation. These operations are learnable through parameters (gamma and beta) to allow the model to adjust \n",
        "    the normalization.\n",
        "\n",
        "Layer Normalization : \n",
        "(1) Layer Normalization, on the other hand, is a technique used to normalize the activations of a layer across \n",
        "    the entire layer, independently for each sample in the batch.\n",
        "(2) It is often used in recurrent neural networks (RNNs) where the concept of “batch” may not be well-defined \n",
        "    due to varying sequence lengths or when the batch size is limited to one.\n",
        "(3) LN normalizes the activations by subtracting the mean and dividing by the standard deviation, computed across\n",
        "    the last dimension (i.e., the dimension representing the features in the layer). The normalization is done per\n",
        "    training example.\n",
        "(4) LN acts on the last dimension of the input tensor, making it more suitable for models that operate on sequences\n",
        "    or when the batch size is small or constrained.\n",
        "\n",
        "    (Source - https://medium.com/@prudhviraju.srivatsavaya/layer-normalisation-and-batch-normalisation-c315ffe9a84b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pMnZ6z52xLa"
      },
      "source": [
        "Q 9)What are regularization techniques in machine learning?(200 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Regularization in machine learning is a set of techniques used to ensure that a machine learning model can\n",
        "generalize to new data within the same data set. These techniques can help reduce the impact of noisy data that\n",
        "falls outside the expected range of patterns. Regularization can also improve the model by making it easier to \n",
        "detect relevant edge cases within a classification task. Regularization is used to strike a balance between model\n",
        "complexity and performance, adeptly steering clear of both underfitting and overfitting. Regularization works by \n",
        "adding a penalty or complexity term or shrinkage term with Residual Sum of Squares (RSS) to the complex model.\n",
        "\n",
        "There are two types of Regularization techniques :\n",
        "(1) Lasso Regression (L1 Regularization).\n",
        "(2) Ridge Regression (L2 Regularization).\n",
        "\n",
        "(Source - https://www.analyticsvidhya.com/blog/2021/05/complete-guide-to-regularization-techniques-in-machine-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoVZoLpB3B7X"
      },
      "source": [
        "Q 10)What is dropout layer and explain how it prevents overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Dropout is a regularization technique that prevents neural networks from overfitting. Regularization methods like\n",
        "L1 and L2 reduce overfitting by modifying the cost function but on the contrary, the Dropout technique modifies\n",
        "the network itself to prevent the network from overfitting. It randomly drops some neurons except for the output\n",
        "layer from the neural network during training in each iteration or we can assign a probability p to all the neurons\n",
        "in a network so that they are temporarily ignored from calculations. It randomly drops some neurons except for the\n",
        "output layer from the neural network during training in each iteration or we can assign a probability p(Dropout Rate) \n",
        "to all the neurons in a network so that they are temporarily ignored from calculations.\n",
        "\n",
        "(Source - https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-prevent-overfitting-in-neural-networks-part-2/#:~:text=a%20particular%20technique.-,Dropout,prevent%20the%20network%20from%20overfitting.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMx3OcJ53CBj"
      },
      "source": [
        "Q 11)Explain L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "L1 Regularization :\n",
        "(1) L1 Regularization, also called a lasso regression, adds the “absolute value of magnitude” of the coefficient\n",
        "    as a penalty term to the loss function.\n",
        "(2) Lasso regression also helps us achieve feature selection by penalizing the weights to approximately equal to \n",
        "    zero if that feature does not serve any purpose in the model.\n",
        "\n",
        "L2 Regularization :\n",
        "(1) L2 Regularization, also called a ridge regression, adds the “squared magnitude” of the coefficient as the \n",
        "    penalty term to the loss function.\n",
        "(2) The key aspect of the L2 penalty is the squaring of the coefficients, which tends to reduce their magnitude.\n",
        "\n",
        "Comparing L1 and L2 Regularization :\n",
        "(1) Sparsity : \n",
        "    Sparsity refers to the number of feature coefficients that are reduced to zero, effectively removing them\n",
        "    from the model. L1 regularization, with its absolute value penalty term, inherently promotes sparsity. \n",
        "    In contrast, L2 regularization does not inherently lead to sparsity. Due to its squared penalty term, \n",
        "    L2 regularization shrinks the coefficients towards zero but typically does not set them to zero.\n",
        "    So, L1 regularization is about feature selection whereas L2 regularization is about controlling model \n",
        "    complexity and preventing overfitting.\n",
        "(2) Solution Path : \n",
        "    In L1 regularization, the solution path is piecewise linear, with coefficients hitting zero at certain\n",
        "    values of the regularization parameter λ. This is because the L1 penalty forces coefficients to zero as\n",
        "    λ increases. In the case of L2 regularization, the solution path is more smooth and continuous. As λ \n",
        "    increases, the coefficients gradually shrink towards zero but do not abruptly become zero.\n",
        "\n",
        "\n",
        "(Source - https://www.geeksforgeeks.org/regularization-in-machine-learning/ and \n",
        "https://medium.com/@fernando.dijkinga/explaining-l1-and-l2-regularization-in-machine-learning-2356ee91c8e3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5TGmHuJ3JTh"
      },
      "source": [
        "Q 12)Write about validation accuracy and why we need it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Validation accuracy measures the proportion of correctly classified instances in the validation set.\n",
        "\n",
        "Validation accuracy is an essential metric for evaluating a machine learning model's performance and ability\n",
        "to generalize. It helps ensure the model doesn't overfit the training data, aids in hyperparameter tuning, \n",
        "guides model selection, and monitors training progress. By prioritizing validation accuracy, practitioners\n",
        "can develop models that perform effectively on real-world, unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCOGr8vl3JXY"
      },
      "source": [
        " Q 13)What do you mean by data augmentation and explain is advantages and disadvantages in detail?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Data augmentation is a technique of artificially increasing the training set by creating modified copies of\n",
        "a dataset using existing data. It includes making minor changes to the dataset or using deep learning to \n",
        "generate new data points.  \n",
        "\n",
        "Advantages : \n",
        "(1) It prevents models from overfitting.\n",
        "(2) Reduces the operational cost of labeling and cleaning the raw dataset. \n",
        "(3) It improves the model accuracy.\n",
        "\n",
        "Disadvantages : \n",
        "(1) The biases in the original dataset persist in the augmented data.\n",
        "(2) Quality assurance for data augmentation is expensive. \n",
        "(3) Finding an effective data augmentation approach can be challenging. \n",
        "\n",
        "(Source - https://www.datacamp.com/tutorial/complete-guide-data-augmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeYHJXKh3O1x"
      },
      "source": [
        "Q 14)What is transfer learning.Explain in detail? Mention various pre-trained model present in the community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "In transfer learning, the knowledge of an already trained machine learning model is applied to a different\n",
        "but related problem. With transfer learning, we basically try to exploit what has been learned in one task\n",
        "to improve generalization in another. We transfer the weights that a network has learned at “task A” to a \n",
        "new “task B.” The early and central layers are employed in transfer learning, and the latter layers are only\n",
        "retrained. It makes use of the labelled data from the task it was trained on.\n",
        "\n",
        "Advantages of transfer learning : \n",
        "(1) Reduced training time, improved neural network performance (in most circumstances), and the absence of \n",
        "    a large amount of data.\n",
        "(2) To train a neural model from scratch, a lot of data is typically needed, but access to that data isn’t \n",
        "    always possible – this is when transfer learning comes in handy. Because the model has already been \n",
        "    pre-trained, a good machine learning model can be generated with fairly little training data using \n",
        "    transfer learning.\n",
        "\n",
        "Some examples of pre-trained models are : \n",
        "(1) BERT (Bidirectional Encoder Representations from Transformers)\n",
        "(2) GPT-2 (Generative Pretrained Transformer 2)\n",
        "(3) ELMo (Embeddings from Language Models)\n",
        "(4) RoBERTa (Robustly Optimized BERT)\n",
        "\n",
        "(Source - https://www.geeksforgeeks.org/top-5-pre-trained-models-in-natural-language-processing-nlp/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0x7E-Cg3Vzc"
      },
      "source": [
        "Q 15) Explain the bias- variance tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "The bias is known as the difference between the prediction of the values by the Machine Learning model\n",
        "and the correct value.\n",
        "\n",
        "The variability of model prediction for a given data point which tells us the spread of our data is \n",
        "called the variance of the model.\n",
        "\n",
        "The bias–variance tradeoff describes the relationship between a model's complexity, the accuracy of its predictions,\n",
        "and how well it can make predictions on previously unseen data that were not used to train the model. In general, \n",
        "as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training\n",
        "data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater\n",
        "variance to the model fit each time we take a set of samples to create a new training data set. It is said that \n",
        "there is greater variance in the model's estimated parameters.\n",
        "\n",
        "(Source - https://www.geeksforgeeks.org/ml-bias-variance-trade-off/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUU9eDCa3S2U"
      },
      "source": [
        "Q 16)Assume you have dataset of patients who visited AIIMS(Delhi) between the period of 2018-2020.The datasets include features from CBC reports,IGE,weight,temp,etc.There problems were mainly classified into gastrointestinal problems, heart problems, diabetes and misc.\n",
        "You being a experienced ML engineer, the hospital has approached you to make a model which given these features can predict the problem that the patient is suffering.You can either train a separate neural network for each of the diseases or to train a single neural network\n",
        "with one output neuron for each disease. Which method do you prefer.Justify your answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "I would prefer to train a single neural network with one output neuron for each disease. The reason being : \n",
        "\n",
        "(1) Shared Learning and Feature Extraction : \n",
        "    The data provided can be used efficiently as many features of the dataset like CBC reports, IGE, weight, temp, etc.\n",
        "    might be relevant for predicting multiple diseases. Also, using all the available collectively helps generalize it \n",
        "    better as compared to training separately.\n",
        "\n",
        "(2) Simpler and Faster Training : \n",
        "    Training a single model is computationally more efficient than training multiple separate models. It also reduces\n",
        "    the training time and resources. Also handling the model, i.e. updating/changing something is easier for a single\n",
        "    neural network.\n",
        "\n",
        "(3) Unified Output : \n",
        "    A single model ensures that the predictions are consistent and aligned, which might not always be the case with\n",
        "    separate models. For example, if separate models were used, they might produce conflicting predictions for a\n",
        "    single patient, making it harder to interpret the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRbqSHnH0Cu5"
      },
      "source": [
        "Q 17) Assume your input data X has m observations and n0 features (shape: n0 x m), and the output layer Y has shape (n3 x m). Say that you want to have 2 hidden layers in your model: A1- (n1 x m) and A2- (n2 x m).\n",
        "\n",
        "(a) How many weight and bias matrices will you have? Find the shapes of each & verify if the dimensions of matrix multiplications are accurate.\n",
        "\n",
        "(b) If this is a Binary classification problem, what activation functions will you use for (i) the hidden layers, and (ii) the output layer. Why?\n",
        "\n",
        "(c) Repeat part (b) if this is a Multi-class classification problem.\n",
        "\n",
        "(d) Repeat part (b) if this is a Regression problem.\n",
        "\n",
        "-> Give proper reasoning for each part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "(a) To compute the number of weight and bias matrices :\n",
        "    (i) Between input layer and A1 hidden layer :\n",
        "        W1 = (n1, n0) and b1 = (n1, 1)\n",
        "    (i) Between A1 hidden layer and A2 hidden layer :\n",
        "        W2 = (n2, n1) and b2 = (n2, 1)\n",
        "    (i) Between A2 hidden layer and output layer :\n",
        "        W1 = (n3, n2) and b1 = (n2, 1)\n",
        "\n",
        "    So there are 3 weights and 3 biases.\n",
        "\n",
        "    To verify we can use the eq. for neural network given by, y = W*X + b, and obverse that it satisfies the condition \n",
        "    of matrix multiplication i.e. no. of columns of 1st matrix must be to no. of rows of 2nd matrix.\n",
        "\n",
        "(b) For the hidden layers i would use ReLU as it introduces non-linearity thereby making the model capable of learning \n",
        "    complex patterns and avoids vanishing gradient problem. For the output layer i would use sigmoid as it a binary \n",
        "    classification problem and sigmoid function squishes the ouput b/w 0 and 1 thereby making it easier to predict for \n",
        "    two classes.\n",
        "\n",
        "(c) Again using ReLU is a justified choice for hidden layers. But for the output layer, using softmax would be better \n",
        "    as this a multiclass classification problem and softmax gives the output in one hot encoding format which sum up \n",
        "    to one making it equivalent to sigmoid for multiple classes.\n",
        "\n",
        "(d) ReLU again for hidden layers. For the output layer, we can use a linear activation function without limiting the\n",
        "    output value range as the output would be in continuous format.\n",
        "\n",
        "    (Source - https://stackoverflow.com/questions/46680481/activation-function-for-output-layer-for-regression-models-in-neural-networks)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
